# 模型

1.小物体可以考虑四个head
2.含attention的backbone，但是用不了预训练参数，需要从其他地方找coco预训练参数，比如SENet、GCNet
3.可变性卷积，可以在backbone最后三层加，难以导出onnx
4.对于大物体较差时可以使用空洞卷积
5.Group Normalization对于batch没有联系

# 训练

1. 加长训练时间，使用3x_schedule
2. 一般论文拆分StepLrSchedule，可以考虑CosineLrSchedule或CosineAnnealingLrSchedule,一般需要配合warmup
3. stochastic weight averaging
4. 大学习率+梯度裁剪，连续n个epochs的val acc不上升或val loss不下降时学习率衰减
5. Label Smoothing是一个正则化方法
6. 在需要量化的场景，可以直接使用低精度（FP16，混合精度）进行训练。
7. 使用更大的batch size，一般也需要等比例增大学习率；

# 数据增强

3.对极小物体检测时，裁剪有助于收敛
4.albumentations库
5.mixup
6.填鸭式，把一些误检的目标放到没有目标的图上的比较合理的位置，做法是比对图像块的互信息，参考[填鸭式](https://github.com/chuliuT/Tianchi_Fabric_defects_detection/blob/master/final_commit/Duck_inject.py)
7.filp，resize，distort，blur
8.mosaic，
9.分类：AutoAugment，Colorjit

# 评估

1.coco和voc的评测方法，会把confiden设置较低，是为拉长PR曲线，使MAP变高
2.不同的NMS算法效果也不同，见[https://github.com/ZFTurbo/Weighted-Boxes-Fusion/tree/master](https://github.com/ZFTurbo/Weighted-Boxes-Fusion/blob/master/benchmark_oid/README.md),WBF效果好但速度慢3倍
3.TTA

# 损失函数

在线难样本学习：

    对loss进行加权，或者删去简单样本的损失

目标检测：
    Bbox loss (IOU,GIOU,DIOU,CIOU)
    Confidence loss(YOLOv4,YOLOv5,PP-YOLO)
    IOU_Aware_Loss(PP-YOLO)
    Focal loss
    IOU Loss：考虑了重叠面积，归一化坐标尺度；
    GIOU Loss：考虑了重叠面积，基于IOU解决边界框不相交时loss等于0的问题；
    DIOU Loss：考虑了重叠面积和中心点距离，基于IOU解决GIOU收敛慢的问题；
    CIOU Loss：考虑了重叠面积、中心点距离、纵横比，基于DIOU提升回归精确度；
    EIOU Loss：考虑了重叠面积，中心点距离、长宽边长真实差，基于CIOU解决了纵横比的模糊定义，并添加Focal Loss解决BBox回归中的样本不平衡问题。

# free bag

  RepConv 重参数化
  粗辅助头和细引导头（yolov7）

   隐式知识结合卷积特征映射和乘法方式（YOLOR）
  EMA模型

  知识蒸馏(knowledge distillation)

# 轻量化

- 卷积核：

1. 大卷积核用多个小卷积核平替
2. 单一尺寸卷积核用多尺寸卷积核平替
3. 可变性卷积
4. 1x1卷积，bottleneck架构

- 通道

1. 深度可分离卷积
2. 分组卷积+channels shuffle
3. 通道加权

- 连接

1. 使用残差连接
2. 融合多层特征


# 加速训练

1. 使用梯度裁剪

   ```python
   loss.backward()
   torch.nn.utils.clip_grad_norm_(model.parameters(), 20) # 在代码中加入这行实现梯度裁剪
   optimizer.step()
   ```
2. 设置torch.backends.cudnn.benchmark = True

  设置**torch.backends.cudnn.benchmark = True**将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速。适用场景：网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）固定。反之，如果卷积层的设置一直变化，将会导致程序不停地做优化，反而会耗费更多的时间。
3. DataLoader
  - pin_memory=True，锁页内存，内存够用一定要加上
  - prefetch_factor=任意数字,预加载
  - persistent_workers=True，训练完1个epoch后，不会重新初始化workers

# 记录

1. 从adam换成sgd后，学习率需要调整，特别是在没有预训练下，更需要调大
2. sgd的动量调整为0.9更易于学习
3. 从0搭建训练流程时，weight_decay可以先设0，因为需要**优先确认**模型能不能收敛或者过拟合
4. 小目标检测，可以增大训练尺寸，上采样用carafe算子
5. 数据集小的时候，可以使用泊松融合生成新图片
